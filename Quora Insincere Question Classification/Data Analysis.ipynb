{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "from scipy.stats import truncnorm\n",
    "import tensorflow as tf\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"emb_dim\":300\n",
    "    ,\"maxWords\": 30\n",
    "    , \"minWordFreq\":10\n",
    "    , \"maxWordFreq\": 10000\n",
    "    , \"WeightMultiplierPosClass\": 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(list(wordCount.values()))\n",
    "data = np.array(list(wordCount.values()))\n",
    "index = list(np.where(np.array(list(wordCount.values())) <= 10)[0])\n",
    "#list(wordCount.keys())[8]\n",
    "data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GloveEmbeddingsIndex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6281a9bb2c9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mGloveEmbeddingsIndex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'GloveEmbeddingsIndex' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = pd.DataFrame({\"ID\":[1,2],\"Id1\":[3,4]})\n",
    "\n",
    "for index,item in enumerate(tmp.iterrows()):\n",
    "    #item[1][\"ID\"] = 10\n",
    "    tmp.iloc\n",
    "\n",
    "GloveEmbeddingsIndex[\":\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanseDataset(data,IS_TRAIN):\n",
    "    global stop_words,config\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data[\"question_text\"] = (data[\"question_text\"].str.replace(u'\\xa0', u' ')\n",
    "                             .replace(\"n't\", ' not ')\n",
    "                             .replace(\"n's\", '')\n",
    "                             .replace(u'.', ' eos ')\n",
    "                            )\n",
    "    #data[\"question_text\"] = data[\"question_text\"].str.replace(\"n't\", ' not ')\n",
    "    #data[\"question_text\"] = data[\"question_text\"].str.replace(\"n's\", '')\n",
    "    #data[\"question_text\"] = data[\"question_text\"].str.replace(u'.', ' eos ')\n",
    "    data[\"wordList\"] = data[\"question_text\"].apply(lambda x: re.split('\\W+', x))\n",
    "    data[\"wordList\"] = data[\"wordList\"].apply(lambda x: [lemmatizer.lemmatize(str.lower(word))\n",
    "                                                     for word in x \n",
    "                                                     if (str.lower(word) not in stop_words) & (len(word) > 1)])\n",
    "    data[\"countWords\"] = data[\"wordList\"].apply(lambda x: len(x))\n",
    "\n",
    "    return data\n",
    "\n",
    "def removeWordsLessThanThreshold(data,wordCount):\n",
    "    global config\n",
    "    \n",
    "    wordListWithLessFreq = []\n",
    "    columnList = []\n",
    "    #---------Make a list of those words which are present less than specified threshold\n",
    "    minWordFreq = config[\"minWordFreq\"]\n",
    "    maxWordFreq = config[\"maxWordFreq\"]\n",
    "    index = 0\n",
    "    for row in data.iterrows():\n",
    "        newWordList = []\n",
    "        wordList = row[1][\"wordList\"]\n",
    "        #pdb.set_trace()\n",
    "        for word in wordList:\n",
    "            if (wordCount[word] > minWordFreq) & (wordCount[word] < maxWordFreq):\n",
    "                newWordList.append(word)\n",
    "        columnList.append(newWordList)\n",
    "        #row[1][\"wordList\"] = newWordList\n",
    "        #pdb.set_trace()\n",
    "        index += 1\n",
    "        if (index % 1000000) == 0:\n",
    "            print(\"Index {}\".format(index))\n",
    "    \n",
    "    data[\"wordList\"] = columnList\n",
    "    data[\"countWords\"] = data[\"wordList\"].apply(lambda x: len(x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def makeFixLengthWordVector(data,IS_TRAIN):\n",
    "    global config\n",
    "    \n",
    "    maxWordsPerStatement = config[\"maxWords\"]\n",
    "    \n",
    "    if IS_TRAIN:\n",
    "        data = data[~((data[\"countWords\"] < 1) | (data[\"countWords\"] > maxWordsPerStatement))]\n",
    "    \n",
    "    #--------Make sure entire statement is of 30 words\n",
    "    data[\"wordList\"] = data[\"wordList\"].apply(lambda x: \n",
    "                                             [\"zerovec\"] * ((maxWordsPerStatement - len(x)) if (maxWordsPerStatement > len(x)) else 0)\n",
    "                                             + x[0:min(len(x),maxWordsPerStatement)]\n",
    "                                             )\n",
    "    return data\n",
    "    \n",
    "def getWordIndexMappingDictionary(wordCount):\n",
    "    import collections\n",
    "    \n",
    "    uniqueWords = set(list(wordCount.keys()) + [\"zerovec\",\"eos\"])\n",
    "    \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    def assignWords_Ids(word,index):\n",
    "        word2idx[word] = index\n",
    "        idx2word[index] = word\n",
    "        \n",
    "    for index,word in enumerate(uniqueWords):\n",
    "        assignWords_Ids(word,index)\n",
    "            \n",
    "    return word2idx,idx2word\n",
    "\n",
    "def assignIdsToWords(data,word2idx):\n",
    "    data[\"WordIDList\"] = data[\"wordList\"].apply(lambda x: [word2idx[word] for word in x])\n",
    "    return data\n",
    "\n",
    "def loadEmbeddings(embeddingfile,word2idx):\n",
    "    global config\n",
    "    \n",
    "    GloveEmbeddingsIndex = {}\n",
    "    emb_dim = config[\"emb_dim\"]\n",
    "    fe = open(embeddingfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")\n",
    "    for index, line in enumerate(fe):\n",
    "        tokens= line.strip().split()\n",
    "        word = tokens[0]\n",
    "        try:\n",
    "            vec = list(map(float,tokens[1:]))\n",
    "            if word in word2idx:\n",
    "                GloveEmbeddingsIndex[word]=vec\n",
    "        except:\n",
    "            print(\"Exception occured: \" + str(index))\n",
    "            \n",
    "    #Add Zerovec, this will be useful to pad zeros, it is better to experiment with padding any non-zero constant values also.\n",
    "    #GloveEmbeddings[\"zerovec\"] = \"0.0 \"*emb_dim\n",
    "    GloveEmbeddingsIndex[\"zerovec\"] = [0.0] *emb_dim\n",
    "    GloveEmbeddingsIndex[\"eos\"] = [0.0] *emb_dim\n",
    "    fe.close()\n",
    "    return GloveEmbeddingsIndex\n",
    "    \n",
    "def getWordsCollection(train,test):\n",
    "    trainWords = [a for b in train[\"wordList\"].tolist() for a in b]\n",
    "    testWords = [a for b in test[\"wordList\"].tolist() for a in b]\n",
    "    words = trainWords + testWords\n",
    "    wordCount = collections.Counter(words)\n",
    "    return wordCount\n",
    "\n",
    "def assignEmbeddingsToWords(word2idx,GloveEmbeddingsIndex):\n",
    "    emb_dim = config[\"emb_dim\"]\n",
    "    vocab_size = len(word2idx)\n",
    "    GloveEmbeddings = np.zeros((vocab_size,emb_dim))\n",
    "    for key,value in word2idx.items():\n",
    "        if(key not in GloveEmbeddingsIndex):\n",
    "            GloveEmbeddings[value] = np.random.normal(0,.1,emb_dim)\n",
    "        else:\n",
    "            GloveEmbeddings[value] = GloveEmbeddingsIndex[key]\n",
    "    return GloveEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "train = cleanseDataset(train,True)\n",
    "test = cleanseDataset(test,False)\n",
    "wordCount = getWordsCollection(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>wordList</th>\n",
       "      <th>countWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>001689a792974397f88f</td>\n",
       "      <td>Why did my clock stop at 3:00 in the morning?</td>\n",
       "      <td>0</td>\n",
       "      <td>[clock, stop, 00, morning]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>00190731e5d576d93154</td>\n",
       "      <td>Who is likely to be fluent in English: a Ghana...</td>\n",
       "      <td>0</td>\n",
       "      <td>[likely, fluent, english, ghanaian, nigerian]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0031884048d46eea1e7f</td>\n",
       "      <td>Who will win in video game battle: Adam d ange...</td>\n",
       "      <td>1</td>\n",
       "      <td>[win, video, game, battle, adam, angelo, v, ki...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>003df1cd16501c899cff</td>\n",
       "      <td>How does your personal knowledge and experienc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[personal, knowledge, experience, affect, way,...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>004913683f771ac568d3</td>\n",
       "      <td>How does your personal knowledge and experienc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[personal, knowledge, experience, affect, way,...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>004e76ae4069b9632dd0</td>\n",
       "      <td>Parents: Have you ever temporarily lost your y...</td>\n",
       "      <td>0</td>\n",
       "      <td>[parent, ever, temporarily, lost, young, child...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>004f17a44eddd8f0a038</td>\n",
       "      <td>If you had to vote between: Mortal Kombat and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[vote, mortal, kombat, grand, theft, auto, wou...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>005a84aff2a194793a0c</td>\n",
       "      <td>Whats the possible causes of the following beh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[whats, possible, cause, following, behavior, ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>006ad579fe95ac7955c8</td>\n",
       "      <td>What is the \"Maximum Ride: School's Out Foreve...</td>\n",
       "      <td>0</td>\n",
       "      <td>[maximum, ride, school, forever, book, james, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339</th>\n",
       "      <td>0074d1015a795543628e</td>\n",
       "      <td>\"Car dry cleaning &amp; car cleaning and detailing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[car, dry, cleaning, car, cleaning, detailing,...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>007c0d90dc2a55805f0f</td>\n",
       "      <td>Why wo not  liberals stop the Trump protests a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[wo, liberal, stop, trump, protest, something,...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>007d7b1e03a204187088</td>\n",
       "      <td>Truth vs Lie: Which is more important componen...</td>\n",
       "      <td>0</td>\n",
       "      <td>[truth, v, lie, important, component, life]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>00880af0490f9feb2363</td>\n",
       "      <td>Do you think Star Trek: Discovery is a bad show?</td>\n",
       "      <td>0</td>\n",
       "      <td>[think, star, trek, discovery, bad, show]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>00a226d418b37938e4dd</td>\n",
       "      <td>I am getting this error when I am trying to sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[getting, error, trying, share, recent, blog, ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>00a582071080999b4e2f</td>\n",
       "      <td>So let me get this straight: Obama and Hillary...</td>\n",
       "      <td>1</td>\n",
       "      <td>[let, get, straight, obama, hillary, give, 147...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>00a9b9efae42582cc701</td>\n",
       "      <td>Ultimate Corruption Showdown: Who is more corr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ultimate, corruption, showdown, corrupt, nawa...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343</th>\n",
       "      <td>00d8c37e2b0f8648e83d</td>\n",
       "      <td>In Mario Puzo’s The Godfather, one (of many) m...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mario, puzo, godfather, one, many, memorable,...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4636</th>\n",
       "      <td>00e7ba8d91979122a480</td>\n",
       "      <td>What should I prefer at intermediate level Bad...</td>\n",
       "      <td>0</td>\n",
       "      <td>[prefer, intermediate, level, badminton, yonex...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>00f52e71a88f998003e0</td>\n",
       "      <td>(WARNING: Anonymous troll!) Do progressives no...</td>\n",
       "      <td>1</td>\n",
       "      <td>[warning, anonymous, troll, progressive, under...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>00f6816fe98c2dac7822</td>\n",
       "      <td>Which is better: mechanical engineering at Del...</td>\n",
       "      <td>0</td>\n",
       "      <td>[better, mechanical, engineering, delhi, unive...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5021</th>\n",
       "      <td>00f96f96163ed88d8ea1</td>\n",
       "      <td>According to the news: So the meats that have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[according, news, meat, cooked, pan, utensil, ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5077</th>\n",
       "      <td>00fcc453256dd6f7c798</td>\n",
       "      <td>Which accent is closer to the English accent o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[accent, closer, english, accent, 1775, anglo,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5132</th>\n",
       "      <td>00ff8083baf5a1595091</td>\n",
       "      <td>If you had to solve this equation: x square pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>[solve, equation, square, plus, one, equal, fi...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5705</th>\n",
       "      <td>011b834f1e076f7ccc20</td>\n",
       "      <td>Which nation took more land from the Khmer Emp...</td>\n",
       "      <td>0</td>\n",
       "      <td>[nation, took, land, khmer, empire, thailand, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6021</th>\n",
       "      <td>012aace620514b14fef0</td>\n",
       "      <td>What does the Avengers: Infinity War ending mean?</td>\n",
       "      <td>0</td>\n",
       "      <td>[avenger, infinity, war, ending, mean]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6395</th>\n",
       "      <td>013d5c8ad54bb23dbcf6</td>\n",
       "      <td>Where is it better to work as a computer scien...</td>\n",
       "      <td>0</td>\n",
       "      <td>[better, work, computer, science, fresher, ind...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6709</th>\n",
       "      <td>014df9faa02359e4f8f0</td>\n",
       "      <td>Anonymous: Why do people think that I'm gay ju...</td>\n",
       "      <td>1</td>\n",
       "      <td>[anonymous, people, think, gay, guy, like, guy...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>015397902d24040c8f2e</td>\n",
       "      <td>South Park: Why does Liane Cartman have no fri...</td>\n",
       "      <td>0</td>\n",
       "      <td>[south, park, liane, cartman, friend]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>01539942020b3eef39ae</td>\n",
       "      <td>Break-up advice: Should I post my Snapchat sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[break, advice, post, snapchat, story, ex]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>01547dae302561d1e36b</td>\n",
       "      <td>Which should one choose: BMSCE Bangalore CSE o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[one, choose, bmsce, bangalore, cse, thapar, u...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299721</th>\n",
       "      <td>febbc12d574ac4d303a3</td>\n",
       "      <td>What would a 1:1 tax break ratio look like, an...</td>\n",
       "      <td>0</td>\n",
       "      <td>[would, tax, break, ratio, look, like, could, ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299807</th>\n",
       "      <td>febffc14951c12da56cc</td>\n",
       "      <td>Davey reads 45 pages in an hour eos  Sally rea...</td>\n",
       "      <td>0</td>\n",
       "      <td>[davey, read, 45, page, hour, eos, sally, read...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300080</th>\n",
       "      <td>fece9cefd1bcccd1ae15</td>\n",
       "      <td>Is Jordan Peterson embodying the scripture of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[jordan, peterson, embodying, scripture, matth...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300131</th>\n",
       "      <td>fed1852aa41b78cbc23e</td>\n",
       "      <td>How do you interpret 1 Corinthians 11:29? 29 “...</td>\n",
       "      <td>0</td>\n",
       "      <td>[interpret, corinthian, 11, 29, 29, one, eats,...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300506</th>\n",
       "      <td>fee385629cb377532197</td>\n",
       "      <td>Let K be a set of real numbers and f: R-&gt;R suc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[let, set, real, number, eos, value, equal]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300784</th>\n",
       "      <td>fef2c0f5693d46d10b2a</td>\n",
       "      <td>What are some books similar to \"Tea with Dee: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[book, similar, tea, dee, daily, devotional, d...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300867</th>\n",
       "      <td>fef7bec29a9a5f76428d</td>\n",
       "      <td>KVPY SB interview: How much importance is give...</td>\n",
       "      <td>0</td>\n",
       "      <td>[kvpy, sb, interview, much, importance, given,...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300935</th>\n",
       "      <td>fefb62041bfb24882a1f</td>\n",
       "      <td>Is eating a single snack of almonds in the mor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[eating, single, snack, almond, morning, one, ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302031</th>\n",
       "      <td>ff3212c0d9cc91487897</td>\n",
       "      <td>What biblical grounds would a minister have fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[biblical, ground, would, minister, condemning...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302083</th>\n",
       "      <td>ff348abbf639f7755f59</td>\n",
       "      <td>Does BHU provide Political Science as Hons eos...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bhu, provide, political, science, hons, eos, ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302205</th>\n",
       "      <td>ff3a97b17259a9659177</td>\n",
       "      <td>Who has the better ME/M eos Tech in Civil Engg...</td>\n",
       "      <td>0</td>\n",
       "      <td>[better, eos, tech, civil, engg, thapar, unive...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302332</th>\n",
       "      <td>ff40b9f0f9007fb7289a</td>\n",
       "      <td>Which is better: SSC or RAS?</td>\n",
       "      <td>0</td>\n",
       "      <td>[better, ssc, ra]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302342</th>\n",
       "      <td>ff41385d3a3440b220c1</td>\n",
       "      <td>Starcraft 2: If I love playing as Protoss more...</td>\n",
       "      <td>0</td>\n",
       "      <td>[starcraft, love, playing, protoss, enjoy, leg...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302426</th>\n",
       "      <td>ff46c2a9b57beac3918e</td>\n",
       "      <td>\"अहिंसा परमो धर्मः धर्म हिंसा तथैव च: ? Non-vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[अह, परम, धर, धर, तथ, non, violence, ultimate,...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303020</th>\n",
       "      <td>ff6541773e29bf8a5bc3</td>\n",
       "      <td>How old is Lyndis in Fire Emblem: The Blazing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[old, lyndis, fire, emblem, blazing, blade]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303335</th>\n",
       "      <td>ff740f6b444bb38ca60b</td>\n",
       "      <td>What inspired the book \"The Silo Effect: The P...</td>\n",
       "      <td>0</td>\n",
       "      <td>[inspired, book, silo, effect, peril, expertis...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303644</th>\n",
       "      <td>ff843b3046297f17ae6e</td>\n",
       "      <td>Are there any strong female characters in King...</td>\n",
       "      <td>0</td>\n",
       "      <td>[strong, female, character, king, arthur, lege...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303679</th>\n",
       "      <td>ff85fc70b1f0a01e1471</td>\n",
       "      <td>How long does it take you to earn 32 million d...</td>\n",
       "      <td>1</td>\n",
       "      <td>[long, take, earn, 32, million, dollar, asking...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303932</th>\n",
       "      <td>ff91297eff104ac18a47</td>\n",
       "      <td>How does the Black Panther City of the Dead re...</td>\n",
       "      <td>0</td>\n",
       "      <td>[black, panther, city, dead, relates, avenger,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303959</th>\n",
       "      <td>ff92af756868f82d71ec</td>\n",
       "      <td>On your own, how would you break free from tho...</td>\n",
       "      <td>0</td>\n",
       "      <td>[would, break, free, grappling, hold, camel, c...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304172</th>\n",
       "      <td>ff9e7f2fcbf955bc4f3b</td>\n",
       "      <td>What are the settings of the book \"Looking Out...</td>\n",
       "      <td>0</td>\n",
       "      <td>[setting, book, looking, outward, year, crisis...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304240</th>\n",
       "      <td>ffa21d4719a67408f4cd</td>\n",
       "      <td>Are low IQ children doomed? Who do you adore m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[low, iq, child, doomed, adore, low, iq, child...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304320</th>\n",
       "      <td>ffa61e0018d4ea1e4461</td>\n",
       "      <td>Which number comes next in this series: 5, 7, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[number, come, next, series, 11, 19, 35]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304547</th>\n",
       "      <td>ffb188cf7c63c8fdb6b1</td>\n",
       "      <td>I am flying into the US on one airline and fly...</td>\n",
       "      <td>0</td>\n",
       "      <td>[flying, u, one, airline, flying, within, u, a...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304599</th>\n",
       "      <td>ffb41b50926cd3507a80</td>\n",
       "      <td>What is more important according to your exper...</td>\n",
       "      <td>0</td>\n",
       "      <td>[important, according, experience, money, family]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304883</th>\n",
       "      <td>ffc274c1df0fc534eed6</td>\n",
       "      <td>Which of these is the hardest/most stressful p...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hardest, stressful, part, applying, college, ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305631</th>\n",
       "      <td>ffe74049eff7ae76d790</td>\n",
       "      <td>Woman: what products do you use for your skin ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[woman, product, use, skin, hair, care, skin, ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305676</th>\n",
       "      <td>ffe9e8dda08fdf5213e8</td>\n",
       "      <td>How do I be assertive and polite: this Muslim ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[assertive, polite, muslim, boy, gave, meaties...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305714</th>\n",
       "      <td>ffeb8080e29ad502931b</td>\n",
       "      <td>Considering what Al Klein has answered in Quor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[considering, al, klein, answered, quora, logi...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305909</th>\n",
       "      <td>fff5d19b80277d43f7bf</td>\n",
       "      <td>What are the settings of the book \"Switched at...</td>\n",
       "      <td>0</td>\n",
       "      <td>[setting, book, switched, birth, true, story, ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "475      001689a792974397f88f   \n",
       "525      00190731e5d576d93154   \n",
       "999      0031884048d46eea1e7f   \n",
       "1254     003df1cd16501c899cff   \n",
       "1462     004913683f771ac568d3   \n",
       "1564     004e76ae4069b9632dd0   \n",
       "1574     004f17a44eddd8f0a038   \n",
       "1819     005a84aff2a194793a0c   \n",
       "2154     006ad579fe95ac7955c8   \n",
       "2339     0074d1015a795543628e   \n",
       "2474     007c0d90dc2a55805f0f   \n",
       "2507     007d7b1e03a204187088   \n",
       "2731     00880af0490f9feb2363   \n",
       "3254     00a226d418b37938e4dd   \n",
       "3327     00a582071080999b4e2f   \n",
       "3397     00a9b9efae42582cc701   \n",
       "4343     00d8c37e2b0f8648e83d   \n",
       "4636     00e7ba8d91979122a480   \n",
       "4920     00f52e71a88f998003e0   \n",
       "4947     00f6816fe98c2dac7822   \n",
       "5021     00f96f96163ed88d8ea1   \n",
       "5077     00fcc453256dd6f7c798   \n",
       "5132     00ff8083baf5a1595091   \n",
       "5705     011b834f1e076f7ccc20   \n",
       "6021     012aace620514b14fef0   \n",
       "6395     013d5c8ad54bb23dbcf6   \n",
       "6709     014df9faa02359e4f8f0   \n",
       "6815     015397902d24040c8f2e   \n",
       "6816     01539942020b3eef39ae   \n",
       "6830     01547dae302561d1e36b   \n",
       "...                       ...   \n",
       "1299721  febbc12d574ac4d303a3   \n",
       "1299807  febffc14951c12da56cc   \n",
       "1300080  fece9cefd1bcccd1ae15   \n",
       "1300131  fed1852aa41b78cbc23e   \n",
       "1300506  fee385629cb377532197   \n",
       "1300784  fef2c0f5693d46d10b2a   \n",
       "1300867  fef7bec29a9a5f76428d   \n",
       "1300935  fefb62041bfb24882a1f   \n",
       "1302031  ff3212c0d9cc91487897   \n",
       "1302083  ff348abbf639f7755f59   \n",
       "1302205  ff3a97b17259a9659177   \n",
       "1302332  ff40b9f0f9007fb7289a   \n",
       "1302342  ff41385d3a3440b220c1   \n",
       "1302426  ff46c2a9b57beac3918e   \n",
       "1303020  ff6541773e29bf8a5bc3   \n",
       "1303335  ff740f6b444bb38ca60b   \n",
       "1303644  ff843b3046297f17ae6e   \n",
       "1303679  ff85fc70b1f0a01e1471   \n",
       "1303932  ff91297eff104ac18a47   \n",
       "1303959  ff92af756868f82d71ec   \n",
       "1304172  ff9e7f2fcbf955bc4f3b   \n",
       "1304240  ffa21d4719a67408f4cd   \n",
       "1304320  ffa61e0018d4ea1e4461   \n",
       "1304547  ffb188cf7c63c8fdb6b1   \n",
       "1304599  ffb41b50926cd3507a80   \n",
       "1304883  ffc274c1df0fc534eed6   \n",
       "1305631  ffe74049eff7ae76d790   \n",
       "1305676  ffe9e8dda08fdf5213e8   \n",
       "1305714  ffeb8080e29ad502931b   \n",
       "1305909  fff5d19b80277d43f7bf   \n",
       "\n",
       "                                             question_text  target  \\\n",
       "475          Why did my clock stop at 3:00 in the morning?       0   \n",
       "525      Who is likely to be fluent in English: a Ghana...       0   \n",
       "999      Who will win in video game battle: Adam d ange...       1   \n",
       "1254     How does your personal knowledge and experienc...       0   \n",
       "1462     How does your personal knowledge and experienc...       0   \n",
       "1564     Parents: Have you ever temporarily lost your y...       0   \n",
       "1574     If you had to vote between: Mortal Kombat and ...       0   \n",
       "1819     Whats the possible causes of the following beh...       0   \n",
       "2154     What is the \"Maximum Ride: School's Out Foreve...       0   \n",
       "2339     \"Car dry cleaning & car cleaning and detailing...       0   \n",
       "2474     Why wo not  liberals stop the Trump protests a...       1   \n",
       "2507     Truth vs Lie: Which is more important componen...       0   \n",
       "2731      Do you think Star Trek: Discovery is a bad show?       0   \n",
       "3254     I am getting this error when I am trying to sh...       0   \n",
       "3327     So let me get this straight: Obama and Hillary...       1   \n",
       "3397     Ultimate Corruption Showdown: Who is more corr...       0   \n",
       "4343     In Mario Puzo’s The Godfather, one (of many) m...       0   \n",
       "4636     What should I prefer at intermediate level Bad...       0   \n",
       "4920     (WARNING: Anonymous troll!) Do progressives no...       1   \n",
       "4947     Which is better: mechanical engineering at Del...       0   \n",
       "5021     According to the news: So the meats that have ...       0   \n",
       "5077     Which accent is closer to the English accent o...       0   \n",
       "5132     If you had to solve this equation: x square pl...       0   \n",
       "5705     Which nation took more land from the Khmer Emp...       0   \n",
       "6021     What does the Avengers: Infinity War ending mean?       0   \n",
       "6395     Where is it better to work as a computer scien...       0   \n",
       "6709     Anonymous: Why do people think that I'm gay ju...       1   \n",
       "6815     South Park: Why does Liane Cartman have no fri...       0   \n",
       "6816     Break-up advice: Should I post my Snapchat sto...       0   \n",
       "6830     Which should one choose: BMSCE Bangalore CSE o...       0   \n",
       "...                                                    ...     ...   \n",
       "1299721  What would a 1:1 tax break ratio look like, an...       0   \n",
       "1299807  Davey reads 45 pages in an hour eos  Sally rea...       0   \n",
       "1300080  Is Jordan Peterson embodying the scripture of ...       1   \n",
       "1300131  How do you interpret 1 Corinthians 11:29? 29 “...       0   \n",
       "1300506  Let K be a set of real numbers and f: R->R suc...       0   \n",
       "1300784  What are some books similar to \"Tea with Dee: ...       0   \n",
       "1300867  KVPY SB interview: How much importance is give...       0   \n",
       "1300935  Is eating a single snack of almonds in the mor...       0   \n",
       "1302031  What biblical grounds would a minister have fo...       1   \n",
       "1302083  Does BHU provide Political Science as Hons eos...       0   \n",
       "1302205  Who has the better ME/M eos Tech in Civil Engg...       0   \n",
       "1302332                       Which is better: SSC or RAS?       0   \n",
       "1302342  Starcraft 2: If I love playing as Protoss more...       0   \n",
       "1302426  \"अहिंसा परमो धर्मः धर्म हिंसा तथैव च: ? Non-vi...       0   \n",
       "1303020  How old is Lyndis in Fire Emblem: The Blazing ...       0   \n",
       "1303335  What inspired the book \"The Silo Effect: The P...       0   \n",
       "1303644  Are there any strong female characters in King...       0   \n",
       "1303679  How long does it take you to earn 32 million d...       1   \n",
       "1303932  How does the Black Panther City of the Dead re...       0   \n",
       "1303959  On your own, how would you break free from tho...       0   \n",
       "1304172  What are the settings of the book \"Looking Out...       0   \n",
       "1304240  Are low IQ children doomed? Who do you adore m...       1   \n",
       "1304320  Which number comes next in this series: 5, 7, ...       0   \n",
       "1304547  I am flying into the US on one airline and fly...       0   \n",
       "1304599  What is more important according to your exper...       0   \n",
       "1304883  Which of these is the hardest/most stressful p...       0   \n",
       "1305631  Woman: what products do you use for your skin ...       0   \n",
       "1305676  How do I be assertive and polite: this Muslim ...       0   \n",
       "1305714  Considering what Al Klein has answered in Quor...       0   \n",
       "1305909  What are the settings of the book \"Switched at...       0   \n",
       "\n",
       "                                                  wordList  countWords  \n",
       "475                             [clock, stop, 00, morning]           4  \n",
       "525          [likely, fluent, english, ghanaian, nigerian]           5  \n",
       "999      [win, video, game, battle, adam, angelo, v, ki...           9  \n",
       "1254     [personal, knowledge, experience, affect, way,...          14  \n",
       "1462     [personal, knowledge, experience, affect, way,...          14  \n",
       "1564     [parent, ever, temporarily, lost, young, child...          10  \n",
       "1574     [vote, mortal, kombat, grand, theft, auto, wou...           8  \n",
       "1819     [whats, possible, cause, following, behavior, ...          14  \n",
       "2154     [maximum, ride, school, forever, book, james, ...           7  \n",
       "2339     [car, dry, cleaning, car, cleaning, detailing,...          16  \n",
       "2474     [wo, liberal, stop, trump, protest, something,...          16  \n",
       "2507           [truth, v, lie, important, component, life]           6  \n",
       "2731             [think, star, trek, discovery, bad, show]           6  \n",
       "3254     [getting, error, trying, share, recent, blog, ...          17  \n",
       "3327     [let, get, straight, obama, hillary, give, 147...          29  \n",
       "3397     [ultimate, corruption, showdown, corrupt, nawa...           7  \n",
       "4343     [mario, puzo, godfather, one, many, memorable,...          18  \n",
       "4636     [prefer, intermediate, level, badminton, yonex...          11  \n",
       "4920     [warning, anonymous, troll, progressive, under...          21  \n",
       "4947     [better, mechanical, engineering, delhi, unive...           6  \n",
       "5021     [according, news, meat, cooked, pan, utensil, ...          12  \n",
       "5077     [accent, closer, english, accent, 1775, anglo,...           8  \n",
       "5132     [solve, equation, square, plus, one, equal, fi...          13  \n",
       "5705     [nation, took, land, khmer, empire, thailand, ...           7  \n",
       "6021                [avenger, infinity, war, ending, mean]           5  \n",
       "6395     [better, work, computer, science, fresher, ind...          12  \n",
       "6709     [anonymous, people, think, gay, guy, like, guy...          12  \n",
       "6815                 [south, park, liane, cartman, friend]           5  \n",
       "6816            [break, advice, post, snapchat, story, ex]           6  \n",
       "6830     [one, choose, bmsce, bangalore, cse, thapar, u...           8  \n",
       "...                                                    ...         ...  \n",
       "1299721  [would, tax, break, ratio, look, like, could, ...          13  \n",
       "1299807  [davey, read, 45, page, hour, eos, sally, read...          26  \n",
       "1300080  [jordan, peterson, embodying, scripture, matth...           7  \n",
       "1300131  [interpret, corinthian, 11, 29, 29, one, eats,...          16  \n",
       "1300506        [let, set, real, number, eos, value, equal]           7  \n",
       "1300784  [book, similar, tea, dee, daily, devotional, d...           8  \n",
       "1300867  [kvpy, sb, interview, much, importance, given,...          17  \n",
       "1300935  [eating, single, snack, almond, morning, one, ...          12  \n",
       "1302031  [biblical, ground, would, minister, condemning...          12  \n",
       "1302083  [bhu, provide, political, science, hons, eos, ...          12  \n",
       "1302205  [better, eos, tech, civil, engg, thapar, unive...           9  \n",
       "1302332                                  [better, ssc, ra]           3  \n",
       "1302342  [starcraft, love, playing, protoss, enjoy, leg...           9  \n",
       "1302426  [अह, परम, धर, धर, तथ, non, violence, ultimate,...          22  \n",
       "1303020        [old, lyndis, fire, emblem, blazing, blade]           6  \n",
       "1303335  [inspired, book, silo, effect, peril, expertis...          11  \n",
       "1303644  [strong, female, character, king, arthur, lege...           7  \n",
       "1303679  [long, take, earn, 32, million, dollar, asking...          24  \n",
       "1303932  [black, panther, city, dead, relates, avenger,...           8  \n",
       "1303959  [would, break, free, grappling, hold, camel, c...          13  \n",
       "1304172  [setting, book, looking, outward, year, crisis...          11  \n",
       "1304240  [low, iq, child, doomed, adore, low, iq, child...          11  \n",
       "1304320           [number, come, next, series, 11, 19, 35]           7  \n",
       "1304547  [flying, u, one, airline, flying, within, u, a...          16  \n",
       "1304599  [important, according, experience, money, family]           5  \n",
       "1304883  [hardest, stressful, part, applying, college, ...          11  \n",
       "1305631  [woman, product, use, skin, hair, care, skin, ...           9  \n",
       "1305676  [assertive, polite, muslim, boy, gave, meaties...          15  \n",
       "1305714  [considering, al, klein, answered, quora, logi...          12  \n",
       "1305909  [setting, book, switched, birth, true, story, ...          10  \n",
       "\n",
       "[8100 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[np.where(train[\"question_text\"].apply(lambda x: \":\" in x))]\n",
    "#print(train.iloc[15655][\"question_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train = removeWordsLessThanThreshold(train,wordCount)\n",
    "test = removeWordsLessThanThreshold(test,wordCount)\n",
    "wordCount = getWordsCollection(train,test)\n",
    "word2idx, idx2word = getWordIndexMappingDictionary(wordCount)\n",
    "train = makeFixLengthWordVector(train,True)\n",
    "test = makeFixLengthWordVector(test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-a8ed3bb2ee51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGloveEmbeddingsIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadEmbeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./Data/embeddings/glove.840B.300d/glove.840B.300d.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#GloveEmbeddings = assignEmbeddingsToWords(word2idx,GloveEmbeddingsIndex)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#del GloveEmbeddingsIndex,wordCount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-d3fcfb412104>\u001b[0m in \u001b[0;36mloadEmbeddings\u001b[1;34m(embeddingfile, word2idx)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0memb_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"emb_dim\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mfe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddingfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GloveEmbeddingsIndex = loadEmbeddings(\"./Data/embeddings/glove.840B.300d/glove.840B.300d.txt\",word2idx)\n",
    "GloveEmbeddings = assignEmbeddingsToWords(word2idx,GloveEmbeddingsIndex)\n",
    "\n",
    "#del GloveEmbeddingsIndex,wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = assignIdsToWords(train,word2idx)\n",
    "test = assignIdsToWords(test,word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':['a','b', 'c'], 'val':[['val1','val2','val33'],\n",
    "                                               ['val33','val9','val6'],\n",
    "                                               ['val2','val6','val7']]})\n",
    "\n",
    "(list(set([a for b in df.val.tolist() for a in b])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[\"WordIDList\",\"target\"]]\n",
    "test = test[[\"WordIDList\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs_, labels_, keep_prob_\n",
    "\n",
    "def build_embedding_layer(inputs_, vocab_size, embed_size):\n",
    "    \"\"\"\n",
    "    Create the embedding layer\n",
    "    \"\"\"\n",
    "    global GloveEmbeddings\n",
    "    \n",
    "    glove_weights_initializer = tf.constant_initializer(GloveEmbeddings)\n",
    "    embedding = tf.get_variable(\n",
    "                name='embedding_weights', \n",
    "                shape=(vocab_size, embed_size), \n",
    "                initializer=glove_weights_initializer,\n",
    "                trainable=True\n",
    "            )\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    return embed\n",
    "\n",
    "\n",
    "def build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size):\n",
    "    \"\"\"\n",
    "    Create the LSTM layers\n",
    "    \"\"\"\n",
    "    lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]\n",
    "    # Add dropout to the cell\n",
    "    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    \n",
    "    return initial_state, lstm_outputs, cell, final_state\n",
    "\n",
    "def build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate):\n",
    "    \"\"\"\n",
    "    Create the Loss function and Optimizer\n",
    "    \"\"\"\n",
    "    tmpMultiplier = config[\"WeightMultiplierPosClass\"]\n",
    "    weights = tf.multiply(tmpMultiplier, tf.cast(tf.argmax(labels_,-1),tf.int32)) + 1\n",
    "    predictions = tf.contrib.layers.fully_connected(lstm_outputs[:, -1], 2, activation_fn=None)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels = labels_, logits = predictions, weights = weights)\n",
    "    optimzer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return predictions, loss, optimzer\n",
    "\n",
    "\n",
    "def build_accuracy(predictions, labels_):\n",
    "    \"\"\"\n",
    "    Create accuracy\n",
    "    \"\"\"\n",
    "    correct_pred = tf.equal(tf.argmax(predictions), tf.argmax(labels_))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.int32))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def getBatches(data, batch_size):\n",
    "    \n",
    "    L = len(data)\n",
    "        \n",
    "    #this line is just to make the generator infinite, keras needs that    \n",
    "    while True:\n",
    "\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            \n",
    "            X = np.concatenate(data.iloc[batch_start:limit][\"WordIDList\"].values,axis = 0).reshape(-1,config[\"maxWords\"])\n",
    "            Y = np.array([[1,0] if item[0] == 0 else [0,1] for item in data.iloc[batch_start:limit][[\"target\"]].values])\n",
    "            #data.iloc[batch_start:limit][[\"target\"]].values\n",
    "            \n",
    "            yield (X,Y) #a tuple with two numpy arrays with batch_size samples     \n",
    "\n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(model_dir, batch_size, test_x, test_y):\n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    test_acc = []\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "        test_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "        for ii, (x, y) in enumerate(utl.get_batches(test_x, test_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob_: 1,\n",
    "                    initial_state: test_state}\n",
    "            batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "            test_acc.append(batch_acc)\n",
    "        print(\"Test Accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size, \n",
    "                            learning_rate, keep_prob, train, val):\n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    best_acc = 0\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        n_batches = len(train)//batch_size\n",
    "        trainGenerator = getBatches(train,batch_size)\n",
    "        validGenerator = getBatches(val,len(val))\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(initial_state)\n",
    "            \n",
    "            train_acc = []\n",
    "            lossValue = []\n",
    "            for ii in range(n_batches):\n",
    "                (x,y) = next(trainGenerator)\n",
    "                #print(y.shape)\n",
    "                #print(y)\n",
    "                feed = {inputs_: x,\n",
    "                        labels_: y,\n",
    "                        keep_prob_: keep_prob,\n",
    "                        initial_state: state}\n",
    "                loss_, state, _,  batch_acc = sess.run([loss, final_state, optimizer, accuracy], feed_dict=feed)\n",
    "                train_acc.append(batch_acc)\n",
    "                lossValue.append(loss_)\n",
    "                \n",
    "                if (ii + 1) % 100 == 0:\n",
    "                    print(\"{}/{} Train Accuracy: {}, Loss: {}\".format(e+1,ii+1, np.mean(train_acc),np.mean(lossValue)))\n",
    "                        \n",
    "                if ((ii + 1) % n_batches) == 0:\n",
    "                    \n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "                    \n",
    "                    #for index in utl.get_batches(val_x, val_y, batch_size):\n",
    "                    (xx,yy) = next(validGenerator)\n",
    "                    feed = {inputs_: xx,\n",
    "                            labels_: yy,\n",
    "                            keep_prob_: 1,\n",
    "                            initial_state: val_state}\n",
    "                    val_batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(val_batch_acc)\n",
    "                    \n",
    "                    acc = np.mean(val_acc)\n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                          \"Batch: {}/{}...\".format(ii+1, n_batches),\n",
    "                          \"Train Loss: {:.3f}...\".format(loss_),\n",
    "                          \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)),\n",
    "                          \"Val Accuracy: {:.3f}\".format(acc))\n",
    "                \n",
    "                    if best_acc < acc:\n",
    "                        best_acc = acc\n",
    "                        saver.save(sess, \"./checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,val = train_test_split(train,test_size = .2)\n",
    "train = train.reset_index(drop = True)\n",
    "val = val.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100 Train Accuracy: 0.05000000074505806...\n",
      "1/200 Train Accuracy: 0.04749999940395355...\n",
      "1/300 Train Accuracy: 0.05999999865889549...\n",
      "1/400 Train Accuracy: 0.06499999761581421...\n",
      "1/500 Train Accuracy: 0.07400000095367432...\n",
      "1/600 Train Accuracy: 0.07916666567325592...\n",
      "1/700 Train Accuracy: 0.07999999821186066...\n",
      "1/800 Train Accuracy: 0.08250000327825546...\n",
      "1/900 Train Accuracy: 0.08166666328907013...\n",
      "1/1000 Train Accuracy: 0.08250000327825546...\n",
      "1/1100 Train Accuracy: 0.08500000089406967...\n",
      "1/1200 Train Accuracy: 0.08416666835546494...\n",
      "1/1300 Train Accuracy: 0.08576922863721848...\n",
      "1/1400 Train Accuracy: 0.0860714316368103...\n",
      "1/1500 Train Accuracy: 0.08699999749660492...\n",
      "1/1600 Train Accuracy: 0.0846875011920929...\n",
      "1/1700 Train Accuracy: 0.0847058817744255...\n",
      "1/1800 Train Accuracy: 0.0855555534362793...\n",
      "1/1900 Train Accuracy: 0.08605263382196426...\n",
      "1/2000 Train Accuracy: 0.08799999952316284...\n",
      "1/2100 Train Accuracy: 0.08595237880945206...\n",
      "1/2200 Train Accuracy: 0.08636363595724106...\n",
      "1/2300 Train Accuracy: 0.0876086950302124...\n",
      "1/2400 Train Accuracy: 0.08895833045244217...\n",
      "1/2500 Train Accuracy: 0.08940000087022781...\n",
      "1/2600 Train Accuracy: 0.08980768918991089...\n",
      "1/2700 Train Accuracy: 0.08981481194496155...\n",
      "1/2800 Train Accuracy: 0.09017857164144516...\n",
      "1/2900 Train Accuracy: 0.08913793414831161...\n",
      "1/3000 Train Accuracy: 0.08916666358709335...\n",
      "1/3100 Train Accuracy: 0.08822580426931381...\n",
      "1/3200 Train Accuracy: 0.08828125149011612...\n",
      "1/3300 Train Accuracy: 0.0875757560133934...\n",
      "1/3400 Train Accuracy: 0.08794117718935013...\n",
      "1/3500 Train Accuracy: 0.08857142925262451...\n",
      "1/3600 Train Accuracy: 0.08847222477197647...\n",
      "1/3700 Train Accuracy: 0.08945945650339127...\n",
      "1/3800 Train Accuracy: 0.08934210240840912...\n",
      "1/3900 Train Accuracy: 0.08935897797346115...\n",
      "1/4000 Train Accuracy: 0.08937499672174454...\n",
      "1/4100 Train Accuracy: 0.08914633840322495...\n",
      "1/4200 Train Accuracy: 0.08892857283353806...\n",
      "1/4300 Train Accuracy: 0.08906976878643036...\n",
      "1/4400 Train Accuracy: 0.08920454233884811...\n",
      "1/4500 Train Accuracy: 0.08866667002439499...\n",
      "1/4600 Train Accuracy: 0.08836956322193146...\n",
      "1/4700 Train Accuracy: 0.08755318820476532...\n",
      "1/4800 Train Accuracy: 0.08770833164453506...\n",
      "1/4900 Train Accuracy: 0.08765306323766708...\n",
      "1/5000 Train Accuracy: 0.08799999952316284...\n",
      "1/5100 Train Accuracy: 0.08794117718935013...\n",
      "1/5200 Train Accuracy: 0.08817307651042938...\n",
      "1/5300 Train Accuracy: 0.08820755034685135...\n",
      "1/5400 Train Accuracy: 0.08768518269062042...\n",
      "1/5500 Train Accuracy: 0.08763636648654938...\n",
      "1/5600 Train Accuracy: 0.08723214268684387...\n",
      "1/5700 Train Accuracy: 0.08701754361391068...\n",
      "1/5800 Train Accuracy: 0.08689655363559723...\n",
      "1/5900 Train Accuracy: 0.08677966147661209...\n",
      "1/6000 Train Accuracy: 0.08658333122730255...\n",
      "1/6100 Train Accuracy: 0.08598360419273376...\n",
      "1/6200 Train Accuracy: 0.08637096732854843...\n",
      "1/6300 Train Accuracy: 0.08642856776714325...\n",
      "1/6400 Train Accuracy: 0.08617187291383743...\n",
      "1/6500 Train Accuracy: 0.08592307567596436...\n",
      "1/6600 Train Accuracy: 0.0861363634467125...\n",
      "1/6700 Train Accuracy: 0.08597014844417572...\n",
      "1/6800 Train Accuracy: 0.08566176146268845...\n",
      "1/6900 Train Accuracy: 0.08550724387168884...\n",
      "1/7000 Train Accuracy: 0.08550000190734863...\n",
      "1/7100 Train Accuracy: 0.08514084666967392...\n",
      "1/7200 Train Accuracy: 0.08541666716337204...\n",
      "1/7300 Train Accuracy: 0.08547945320606232...\n",
      "1/7400 Train Accuracy: 0.08520270138978958...\n",
      "1/7500 Train Accuracy: 0.0851999968290329...\n",
      "1/7600 Train Accuracy: 0.08532894402742386...\n",
      "1/7700 Train Accuracy: 0.08577921986579895...\n",
      "1/7800 Train Accuracy: 0.08564102649688721...\n",
      "1/7900 Train Accuracy: 0.08544303476810455...\n",
      "1/8000 Train Accuracy: 0.08493749797344208...\n",
      "1/8100 Train Accuracy: 0.08506172895431519...\n",
      "1/8200 Train Accuracy: 0.08518292754888535...\n",
      "1/8300 Train Accuracy: 0.08518072217702866...\n",
      "1/8400 Train Accuracy: 0.08577381074428558...\n",
      "1/8500 Train Accuracy: 0.0860000029206276...\n",
      "1/8600 Train Accuracy: 0.08610465377569199...\n",
      "1/8700 Train Accuracy: 0.08580459654331207...\n",
      "1/8800 Train Accuracy: 0.08579545468091965...\n",
      "1/8900 Train Accuracy: 0.08606741577386856...\n",
      "1/9000 Train Accuracy: 0.08572222292423248...\n",
      "1/9100 Train Accuracy: 0.08565934002399445...\n",
      "1/9200 Train Accuracy: 0.0856521725654602...\n",
      "1/9300 Train Accuracy: 0.0855913981795311...\n",
      "1/9400 Train Accuracy: 0.08585106581449509...\n",
      "1/9500 Train Accuracy: 0.08584210276603699...\n",
      "1/9600 Train Accuracy: 0.0859895870089531...\n",
      "1/9700 Train Accuracy: 0.08628866076469421...\n",
      "1/9800 Train Accuracy: 0.08602041006088257...\n",
      "1/9900 Train Accuracy: 0.08616161346435547...\n",
      "1/10000 Train Accuracy: 0.08619999885559082...\n",
      "1/10100 Train Accuracy: 0.08599010109901428...\n",
      "1/10200 Train Accuracy: 0.08632352948188782...\n",
      "1/10300 Train Accuracy: 0.08601941913366318...\n",
      "1/10400 Train Accuracy: 0.08600961416959763...\n",
      "1/10500 Train Accuracy: 0.0857619047164917...\n",
      "1/10600 Train Accuracy: 0.0856132060289383...\n",
      "1/10700 Train Accuracy: 0.08546728640794754...\n",
      "1/10800 Train Accuracy: 0.08550926297903061...\n",
      "1/10900 Train Accuracy: 0.08550458401441574...\n",
      "1/11000 Train Accuracy: 0.08554545789957047...\n",
      "1/11100 Train Accuracy: 0.08540540188550949...\n",
      "1/11200 Train Accuracy: 0.08540178835391998...\n",
      "1/11300 Train Accuracy: 0.08522123843431473...\n",
      "1/11400 Train Accuracy: 0.08521930128335953...\n",
      "1/11500 Train Accuracy: 0.08500000089406967...\n",
      "1/11600 Train Accuracy: 0.08474137634038925...\n",
      "1/11700 Train Accuracy: 0.08431623876094818...\n",
      "1/11800 Train Accuracy: 0.08419491350650787...\n",
      "1/11900 Train Accuracy: 0.08411764353513718...\n",
      "1/12000 Train Accuracy: 0.08399999886751175...\n",
      "1/12100 Train Accuracy: 0.08384297788143158...\n",
      "1/12200 Train Accuracy: 0.08356557041406631...\n",
      "1/12300 Train Accuracy: 0.08321138471364975...\n",
      "1/12400 Train Accuracy: 0.08298387378454208...\n",
      "1/12500 Train Accuracy: 0.08284000307321548...\n",
      "1/12600 Train Accuracy: 0.08277777582406998...\n",
      "1/12700 Train Accuracy: 0.08248031139373779...\n",
      "1/12800 Train Accuracy: 0.08234374970197678...\n",
      "1/12900 Train Accuracy: 0.0820930227637291...\n",
      "1/13000 Train Accuracy: 0.08230768889188766...\n",
      "1/13100 Train Accuracy: 0.08232824504375458...\n",
      "1/13200 Train Accuracy: 0.08231060951948166...\n",
      "1/13300 Train Accuracy: 0.0825187936425209...\n",
      "1/13400 Train Accuracy: 0.0822761207818985...\n",
      "1/13500 Train Accuracy: 0.08222222328186035...\n",
      "1/13600 Train Accuracy: 0.08213235437870026...\n",
      "1/13700 Train Accuracy: 0.08204379677772522...\n",
      "1/13800 Train Accuracy: 0.08184782415628433...\n",
      "1/13900 Train Accuracy: 0.08187050372362137...\n",
      "1/14000 Train Accuracy: 0.08196428418159485...\n",
      "1/14100 Train Accuracy: 0.08187942951917648...\n",
      "1/14200 Train Accuracy: 0.08204225450754166...\n",
      "1/14300 Train Accuracy: 0.08202797174453735...\n",
      "1/14400 Train Accuracy: 0.08222222328186035...\n",
      "1/14500 Train Accuracy: 0.08213792741298676...\n",
      "1/14600 Train Accuracy: 0.08205479383468628...\n",
      "1/14700 Train Accuracy: 0.08180271834135056...\n",
      "1/14800 Train Accuracy: 0.08192567527294159...\n",
      "1/14900 Train Accuracy: 0.08218120783567429...\n",
      "1/15000 Train Accuracy: 0.08206667006015778...\n",
      "1/15100 Train Accuracy: 0.08192052692174911...\n",
      "1/15200 Train Accuracy: 0.08200658112764359...\n",
      "1/15300 Train Accuracy: 0.08179738372564316...\n",
      "1/15400 Train Accuracy: 0.08159090578556061...\n",
      "1/15500 Train Accuracy: 0.08145160973072052...\n",
      "1/15600 Train Accuracy: 0.08128204941749573...\n",
      "1/15700 Train Accuracy: 0.08098725974559784...\n",
      "1/15800 Train Accuracy: 0.08072784543037415...\n",
      "1/15900 Train Accuracy: 0.08062893152236938...\n",
      "1/16000 Train Accuracy: 0.08034375309944153...\n",
      "1/16100 Train Accuracy: 0.07999999821186066...\n"
     ]
    }
   ],
   "source": [
    "# Define Inputs and Hyperparameters\n",
    "lstm_sizes = [128,64]\n",
    "vocab_size = len(word2idx)  #add one for padding\n",
    "embed_size = config[\"emb_dim\"]\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.concatenate(train.loc[5:10][\"WordIDList\"].values,axis = 0).shape\n",
    "#np.array([[1,0] if item[0] == 0 else [0,1] for item in train[[\"target\"]].head(12).values]).shape\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    test_network('checkpoints', batch_size, test_x, test_y)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
