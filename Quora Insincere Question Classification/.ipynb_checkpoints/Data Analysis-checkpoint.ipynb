{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "from scipy.stats import truncnorm\n",
    "import tensorflow as tf\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"emb_dim\":300\n",
    "    ,\"maxWords\": 30\n",
    "    , \"minWordFreq\":10\n",
    "    , \"maxWordFreq\": 10000\n",
    "    , \"WeightMultiplierPosClass\": 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    1225312\n",
       "1      80810\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted(config.items(), key = lambda x : x[1])\n",
    "#pd.DataFrame({\"wordCount\": list(config.values())}).groupby(\"wordCount\")[\"wordCount\"].sum()\n",
    "#list(config.values())\n",
    "#ps = PorterStemmer()\n",
    "#ps.stem(\"specifically\")\n",
    "tmp = {}\n",
    "tmp.setdefault(\"abc\", 0)\n",
    "tmp[\"abc\"]+=1\n",
    "tmp.setdefault(\"abc\", 0)\n",
    "tmp[\"abc\"]+=1\n",
    "tmp\n",
    "[[1,0] if item[0] == 0 else [0,1] for item in train.iloc[19:23][[\"target\"]].values]\n",
    "#train[train[\"target\"] == 1]\n",
    "\n",
    "train.groupby([\"target\"])[\"target\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(list(wordCount.values()))\n",
    "data = np.array(list(wordCount.values()))\n",
    "index = list(np.where(np.array(list(wordCount.values())) <= 10)[0])\n",
    "#list(wordCount.keys())[8]\n",
    "data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame({\"ID\":[1,2],\"Id1\":[3,4]})\n",
    "\n",
    "for index,item in enumerate(tmp.iterrows()):\n",
    "    #item[1][\"ID\"] = 10\n",
    "    tmp.iloc\n",
    "\n",
    "[\"abc\"] * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanseDataset(data,IS_TRAIN):\n",
    "    global stop_words,config\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data[\"question_text\"] = data[\"question_text\"].str.replace(u'\\xa0', u' ')\n",
    "    data[\"question_text\"] = data[\"question_text\"].str.replace(\"n't\", ' not ')\n",
    "    data[\"question_text\"] = data[\"question_text\"].str.replace(u'.', ' eos ')\n",
    "    data[\"wordList\"] = data[\"question_text\"].apply(lambda x: re.split('\\W+', x))\n",
    "    data[\"wordList\"] = data[\"wordList\"].apply(lambda x: [lemmatizer.lemmatize(str.lower(word))\n",
    "                                                     for word in x \n",
    "                                                     if (str.lower(word) not in stop_words) & (len(word) > 1)])\n",
    "    data[\"countWords\"] = data[\"wordList\"].apply(lambda x: len(x))\n",
    "\n",
    "    return data\n",
    "\n",
    "def removeWordsLessThanThreshold(data,wordCount):\n",
    "    global config\n",
    "    \n",
    "    wordListWithLessFreq = []\n",
    "    columnList = []\n",
    "    #---------Make a list of those words which are present less than specified threshold\n",
    "    minWordFreq = config[\"minWordFreq\"]\n",
    "    maxWordFreq = config[\"maxWordFreq\"]\n",
    "    index = 0\n",
    "    for row in data.iterrows():\n",
    "        newWordList = []\n",
    "        wordList = row[1][\"wordList\"]\n",
    "        #pdb.set_trace()\n",
    "        for word in wordList:\n",
    "            if (wordCount[word] > minWordFreq) & (wordCount[word] < maxWordFreq):\n",
    "                newWordList.append(word)\n",
    "        columnList.append(newWordList)\n",
    "        #row[1][\"wordList\"] = newWordList\n",
    "        #pdb.set_trace()\n",
    "        index += 1\n",
    "        if (index % 1000000) == 0:\n",
    "            print(\"Index {}\".format(index))\n",
    "    \n",
    "    data[\"wordList\"] = columnList\n",
    "    data[\"countWords\"] = data[\"wordList\"].apply(lambda x: len(x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def makeFixLengthWordVector(data,IS_TRAIN):\n",
    "    global config\n",
    "    \n",
    "    maxWordsPerStatement = config[\"maxWords\"]\n",
    "    \n",
    "    if IS_TRAIN:\n",
    "        data = data[~((data[\"countWords\"] < 1) | (data[\"countWords\"] > maxWordsPerStatement))]\n",
    "    \n",
    "    #--------Make sure entire statement is of 30 words\n",
    "    data[\"wordList\"] = data[\"wordList\"].apply(lambda x: \n",
    "                                             [\"zerovec\"] * ((maxWordsPerStatement - len(x)) if (maxWordsPerStatement > len(x)) else 0)\n",
    "                                             + x[0:min(len(x),maxWordsPerStatement)]\n",
    "                                             )\n",
    "    return data\n",
    "    \n",
    "def getWordIndexMappingDictionary(wordCount):\n",
    "    import collections\n",
    "    \n",
    "    uniqueWords = set(list(wordCount.keys()) + [\"zerovec\",\"eos\"])\n",
    "    \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    def assignWords_Ids(word,index):\n",
    "        word2idx[word] = index\n",
    "        idx2word[index] = word\n",
    "        \n",
    "    for index,word in enumerate(uniqueWords):\n",
    "        assignWords_Ids(word,index)\n",
    "            \n",
    "    return word2idx,idx2word\n",
    "\n",
    "def assignIdsToWords(data,word2idx):\n",
    "    data[\"WordIDList\"] = data[\"wordList\"].apply(lambda x: [word2idx[word] for word in x])\n",
    "    return data\n",
    "\n",
    "def loadEmbeddings(embeddingfile,word2idx):\n",
    "    global config\n",
    "    \n",
    "    GloveEmbeddingsIndex = {}\n",
    "    emb_dim = config[\"emb_dim\"]\n",
    "    fe = open(embeddingfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")\n",
    "    for index, line in enumerate(fe):\n",
    "        tokens= line.strip().split()\n",
    "        word = tokens[0]\n",
    "        try:\n",
    "            vec = list(map(float,tokens[1:]))\n",
    "            if word in word2idx:\n",
    "                GloveEmbeddingsIndex[word]=vec\n",
    "        except:\n",
    "            print(\"Exception occured: \" + str(index))\n",
    "            \n",
    "    #Add Zerovec, this will be useful to pad zeros, it is better to experiment with padding any non-zero constant values also.\n",
    "    #GloveEmbeddings[\"zerovec\"] = \"0.0 \"*emb_dim\n",
    "    GloveEmbeddingsIndex[\"zerovec\"] = [0.0] *emb_dim\n",
    "    GloveEmbeddingsIndex[\"eos\"] = [0.0] *emb_dim\n",
    "    fe.close()\n",
    "    return GloveEmbeddingsIndex\n",
    "    \n",
    "def getWordsCollection(train,test):\n",
    "    trainWords = [a for b in train[\"wordList\"].tolist() for a in b]\n",
    "    testWords = [a for b in test[\"wordList\"].tolist() for a in b]\n",
    "    words = trainWords + testWords\n",
    "    wordCount = collections.Counter(words)\n",
    "    return wordCount\n",
    "\n",
    "def assignEmbeddingsToWords(word2idx,GloveEmbeddingsIndex):\n",
    "    emb_dim = config[\"emb_dim\"]\n",
    "    vocab_size = len(word2idx)\n",
    "    GloveEmbeddings = np.zeros((vocab_size,emb_dim))\n",
    "    for key,value in word2idx.items():\n",
    "        if(key not in GloveEmbeddingsIndex):\n",
    "            GloveEmbeddings[value] = np.random.normal(0,.1,emb_dim)\n",
    "        else:\n",
    "            GloveEmbeddings[value] = GloveEmbeddingsIndex[key]\n",
    "    return GloveEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "train = cleanseDataset(train,True)\n",
    "test = cleanseDataset(test,False)\n",
    "wordCount = getWordsCollection(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid                                           00012011b6c7759461e8\n",
       "question_text    Why haven't two democracies never ever went fo...\n",
       "target                                                           0\n",
       "wordList         [two, democracy, never, ever, went, full, fled...\n",
       "countWords                                                       9\n",
       "Name: 26, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train[\"question_text\"].apply(lambda x: \"n't\" in x)\n",
    "train.iloc[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train = removeWordsLessThanThreshold(train,wordCount)\n",
    "test = removeWordsLessThanThreshold(test,wordCount)\n",
    "wordCount = getWordsCollection(train,test)\n",
    "word2idx, idx2word = getWordIndexMappingDictionary(wordCount)\n",
    "train = makeFixLengthWordVector(train,True)\n",
    "test = makeFixLengthWordVector(test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloveEmbeddingsIndex = loadEmbeddings(\"./Data/embeddings/glove.840B.300d/glove.840B.300d.txt\",word2idx)\n",
    "GloveEmbeddings = assignEmbeddingsToWords(word2idx,GloveEmbeddingsIndex)\n",
    "\n",
    "#del GloveEmbeddingsIndex,wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = assignIdsToWords(train,word2idx)\n",
    "test = assignIdsToWords(test,word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':['a','b', 'c'], 'val':[['val1','val2','val33'],\n",
    "                                               ['val33','val9','val6'],\n",
    "                                               ['val2','val6','val7']]})\n",
    "\n",
    "(list(set([a for b in df.val.tolist() for a in b])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[\"WordIDList\",\"target\"]]\n",
    "test = test[[\"WordIDList\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs_, labels_, keep_prob_\n",
    "\n",
    "def build_embedding_layer(inputs_, vocab_size, embed_size):\n",
    "    \"\"\"\n",
    "    Create the embedding layer\n",
    "    \"\"\"\n",
    "    global GloveEmbeddings\n",
    "    \n",
    "    glove_weights_initializer = tf.constant_initializer(GloveEmbeddings)\n",
    "    embedding = tf.get_variable(\n",
    "                name='embedding_weights', \n",
    "                shape=(vocab_size, embed_size), \n",
    "                initializer=glove_weights_initializer,\n",
    "                trainable=True\n",
    "            )\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "    return embed\n",
    "\n",
    "\n",
    "def build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size):\n",
    "    \"\"\"\n",
    "    Create the LSTM layers\n",
    "    \"\"\"\n",
    "    lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]\n",
    "    # Add dropout to the cell\n",
    "    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    \n",
    "    return initial_state, lstm_outputs, cell, final_state\n",
    "\n",
    "def build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate):\n",
    "    \"\"\"\n",
    "    Create the Loss function and Optimizer\n",
    "    \"\"\"\n",
    "    tmpMultiplier = config[\"WeightMultiplierPosClass\"]\n",
    "    weights = tf.multiply(tmpMultiplier, tf.cast(tf.argmax(labels_,-1),tf.int32)) + 1\n",
    "    predictions = tf.contrib.layers.fully_connected(lstm_outputs[:, -1], 2, activation_fn=None)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels = labels_, logits = predictions, weights = weights)\n",
    "    optimzer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return predictions, loss, optimzer\n",
    "\n",
    "\n",
    "def build_accuracy(predictions, labels_):\n",
    "    \"\"\"\n",
    "    Create accuracy\n",
    "    \"\"\"\n",
    "    correct_pred = tf.equal(tf.argmax(predictions), tf.argmax(labels_))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.int32))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def getBatches(data, batch_size):\n",
    "    \n",
    "    L = len(data)\n",
    "        \n",
    "    #this line is just to make the generator infinite, keras needs that    \n",
    "    while True:\n",
    "\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            \n",
    "            X = np.concatenate(data.iloc[batch_start:limit][\"WordIDList\"].values,axis = 0).reshape(-1,config[\"maxWords\"])\n",
    "            Y = np.array([[1,0] if item[0] == 0 else [0,1] for item in data.iloc[batch_start:limit][[\"target\"]].values])\n",
    "            #data.iloc[batch_start:limit][[\"target\"]].values\n",
    "            \n",
    "            yield (X,Y) #a tuple with two numpy arrays with batch_size samples     \n",
    "\n",
    "            batch_start += batch_size   \n",
    "            batch_end += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(model_dir, batch_size, test_x, test_y):\n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    test_acc = []\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "        test_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "        for ii, (x, y) in enumerate(utl.get_batches(test_x, test_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob_: 1,\n",
    "                    initial_state: test_state}\n",
    "            batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "            test_acc.append(batch_acc)\n",
    "        print(\"Test Accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size, \n",
    "                            learning_rate, keep_prob, train, val):\n",
    "    \n",
    "    inputs_, labels_, keep_prob_ = model_inputs()\n",
    "    embed = build_embedding_layer(inputs_, vocab_size, embed_size)\n",
    "    initial_state, lstm_outputs, lstm_cell, final_state = build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size)\n",
    "    predictions, loss, optimizer = build_cost_fn_and_opt(lstm_outputs, labels_, learning_rate)\n",
    "    accuracy = build_accuracy(predictions, labels_)\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    best_acc = 0\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        n_batches = len(train)//batch_size\n",
    "        trainGenerator = getBatches(train,batch_size)\n",
    "        validGenerator = getBatches(val,len(val))\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(initial_state)\n",
    "            \n",
    "            train_acc = []\n",
    "            lossValue = []\n",
    "            for ii in range(n_batches):\n",
    "                (x,y) = next(trainGenerator)\n",
    "                #print(y.shape)\n",
    "                #print(y)\n",
    "                feed = {inputs_: x,\n",
    "                        labels_: y,\n",
    "                        keep_prob_: keep_prob,\n",
    "                        initial_state: state}\n",
    "                loss_, state, _,  batch_acc = sess.run([loss, final_state, optimizer, accuracy], feed_dict=feed)\n",
    "                train_acc.append(batch_acc)\n",
    "                lossValue.append(loss_)\n",
    "                \n",
    "                if (ii + 1) % 100 == 0:\n",
    "                    print(\"{}/{} Train Accuracy: {}, Loss: {}\".format(e+1,ii+1, np.mean(train_acc),np.mean(lossValue)))\n",
    "                        \n",
    "                if ((ii + 1) % n_batches) == 0:\n",
    "                    \n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(lstm_cell.zero_state(batch_size, tf.float32))\n",
    "                    \n",
    "                    #for index in utl.get_batches(val_x, val_y, batch_size):\n",
    "                    (xx,yy) = next(validGenerator)\n",
    "                    feed = {inputs_: xx,\n",
    "                            labels_: yy,\n",
    "                            keep_prob_: 1,\n",
    "                            initial_state: val_state}\n",
    "                    val_batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(val_batch_acc)\n",
    "                    \n",
    "                    acc = np.mean(val_acc)\n",
    "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                          \"Batch: {}/{}...\".format(ii+1, n_batches),\n",
    "                          \"Train Loss: {:.3f}...\".format(loss_),\n",
    "                          \"Train Accruacy: {:.3f}...\".format(np.mean(train_acc)),\n",
    "                          \"Val Accuracy: {:.3f}\".format(acc))\n",
    "                \n",
    "                    if best_acc < acc:\n",
    "                        best_acc = acc\n",
    "                        saver.save(sess, \"./checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,val = train_test_split(train,test_size = .2)\n",
    "train = train.reset_index(drop = True)\n",
    "val = val.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100 Train Accuracy: 0.05000000074505806...\n",
      "1/200 Train Accuracy: 0.04749999940395355...\n",
      "1/300 Train Accuracy: 0.05999999865889549...\n",
      "1/400 Train Accuracy: 0.06499999761581421...\n",
      "1/500 Train Accuracy: 0.07400000095367432...\n",
      "1/600 Train Accuracy: 0.07916666567325592...\n",
      "1/700 Train Accuracy: 0.07999999821186066...\n",
      "1/800 Train Accuracy: 0.08250000327825546...\n",
      "1/900 Train Accuracy: 0.08166666328907013...\n",
      "1/1000 Train Accuracy: 0.08250000327825546...\n",
      "1/1100 Train Accuracy: 0.08500000089406967...\n",
      "1/1200 Train Accuracy: 0.08416666835546494...\n",
      "1/1300 Train Accuracy: 0.08576922863721848...\n",
      "1/1400 Train Accuracy: 0.0860714316368103...\n",
      "1/1500 Train Accuracy: 0.08699999749660492...\n",
      "1/1600 Train Accuracy: 0.0846875011920929...\n",
      "1/1700 Train Accuracy: 0.0847058817744255...\n",
      "1/1800 Train Accuracy: 0.0855555534362793...\n",
      "1/1900 Train Accuracy: 0.08605263382196426...\n",
      "1/2000 Train Accuracy: 0.08799999952316284...\n",
      "1/2100 Train Accuracy: 0.08595237880945206...\n",
      "1/2200 Train Accuracy: 0.08636363595724106...\n",
      "1/2300 Train Accuracy: 0.0876086950302124...\n",
      "1/2400 Train Accuracy: 0.08895833045244217...\n",
      "1/2500 Train Accuracy: 0.08940000087022781...\n",
      "1/2600 Train Accuracy: 0.08980768918991089...\n",
      "1/2700 Train Accuracy: 0.08981481194496155...\n",
      "1/2800 Train Accuracy: 0.09017857164144516...\n",
      "1/2900 Train Accuracy: 0.08913793414831161...\n",
      "1/3000 Train Accuracy: 0.08916666358709335...\n",
      "1/3100 Train Accuracy: 0.08822580426931381...\n",
      "1/3200 Train Accuracy: 0.08828125149011612...\n",
      "1/3300 Train Accuracy: 0.0875757560133934...\n",
      "1/3400 Train Accuracy: 0.08794117718935013...\n",
      "1/3500 Train Accuracy: 0.08857142925262451...\n",
      "1/3600 Train Accuracy: 0.08847222477197647...\n",
      "1/3700 Train Accuracy: 0.08945945650339127...\n",
      "1/3800 Train Accuracy: 0.08934210240840912...\n",
      "1/3900 Train Accuracy: 0.08935897797346115...\n",
      "1/4000 Train Accuracy: 0.08937499672174454...\n",
      "1/4100 Train Accuracy: 0.08914633840322495...\n",
      "1/4200 Train Accuracy: 0.08892857283353806...\n",
      "1/4300 Train Accuracy: 0.08906976878643036...\n",
      "1/4400 Train Accuracy: 0.08920454233884811...\n",
      "1/4500 Train Accuracy: 0.08866667002439499...\n",
      "1/4600 Train Accuracy: 0.08836956322193146...\n",
      "1/4700 Train Accuracy: 0.08755318820476532...\n",
      "1/4800 Train Accuracy: 0.08770833164453506...\n",
      "1/4900 Train Accuracy: 0.08765306323766708...\n",
      "1/5000 Train Accuracy: 0.08799999952316284...\n",
      "1/5100 Train Accuracy: 0.08794117718935013...\n",
      "1/5200 Train Accuracy: 0.08817307651042938...\n",
      "1/5300 Train Accuracy: 0.08820755034685135...\n",
      "1/5400 Train Accuracy: 0.08768518269062042...\n",
      "1/5500 Train Accuracy: 0.08763636648654938...\n",
      "1/5600 Train Accuracy: 0.08723214268684387...\n",
      "1/5700 Train Accuracy: 0.08701754361391068...\n",
      "1/5800 Train Accuracy: 0.08689655363559723...\n",
      "1/5900 Train Accuracy: 0.08677966147661209...\n",
      "1/6000 Train Accuracy: 0.08658333122730255...\n",
      "1/6100 Train Accuracy: 0.08598360419273376...\n",
      "1/6200 Train Accuracy: 0.08637096732854843...\n",
      "1/6300 Train Accuracy: 0.08642856776714325...\n",
      "1/6400 Train Accuracy: 0.08617187291383743...\n",
      "1/6500 Train Accuracy: 0.08592307567596436...\n",
      "1/6600 Train Accuracy: 0.0861363634467125...\n",
      "1/6700 Train Accuracy: 0.08597014844417572...\n",
      "1/6800 Train Accuracy: 0.08566176146268845...\n",
      "1/6900 Train Accuracy: 0.08550724387168884...\n",
      "1/7000 Train Accuracy: 0.08550000190734863...\n",
      "1/7100 Train Accuracy: 0.08514084666967392...\n",
      "1/7200 Train Accuracy: 0.08541666716337204...\n",
      "1/7300 Train Accuracy: 0.08547945320606232...\n",
      "1/7400 Train Accuracy: 0.08520270138978958...\n",
      "1/7500 Train Accuracy: 0.0851999968290329...\n",
      "1/7600 Train Accuracy: 0.08532894402742386...\n",
      "1/7700 Train Accuracy: 0.08577921986579895...\n",
      "1/7800 Train Accuracy: 0.08564102649688721...\n",
      "1/7900 Train Accuracy: 0.08544303476810455...\n",
      "1/8000 Train Accuracy: 0.08493749797344208...\n",
      "1/8100 Train Accuracy: 0.08506172895431519...\n",
      "1/8200 Train Accuracy: 0.08518292754888535...\n",
      "1/8300 Train Accuracy: 0.08518072217702866...\n",
      "1/8400 Train Accuracy: 0.08577381074428558...\n",
      "1/8500 Train Accuracy: 0.0860000029206276...\n",
      "1/8600 Train Accuracy: 0.08610465377569199...\n",
      "1/8700 Train Accuracy: 0.08580459654331207...\n",
      "1/8800 Train Accuracy: 0.08579545468091965...\n",
      "1/8900 Train Accuracy: 0.08606741577386856...\n",
      "1/9000 Train Accuracy: 0.08572222292423248...\n",
      "1/9100 Train Accuracy: 0.08565934002399445...\n",
      "1/9200 Train Accuracy: 0.0856521725654602...\n",
      "1/9300 Train Accuracy: 0.0855913981795311...\n",
      "1/9400 Train Accuracy: 0.08585106581449509...\n",
      "1/9500 Train Accuracy: 0.08584210276603699...\n",
      "1/9600 Train Accuracy: 0.0859895870089531...\n",
      "1/9700 Train Accuracy: 0.08628866076469421...\n",
      "1/9800 Train Accuracy: 0.08602041006088257...\n",
      "1/9900 Train Accuracy: 0.08616161346435547...\n",
      "1/10000 Train Accuracy: 0.08619999885559082...\n",
      "1/10100 Train Accuracy: 0.08599010109901428...\n",
      "1/10200 Train Accuracy: 0.08632352948188782...\n",
      "1/10300 Train Accuracy: 0.08601941913366318...\n",
      "1/10400 Train Accuracy: 0.08600961416959763...\n",
      "1/10500 Train Accuracy: 0.0857619047164917...\n",
      "1/10600 Train Accuracy: 0.0856132060289383...\n",
      "1/10700 Train Accuracy: 0.08546728640794754...\n",
      "1/10800 Train Accuracy: 0.08550926297903061...\n",
      "1/10900 Train Accuracy: 0.08550458401441574...\n",
      "1/11000 Train Accuracy: 0.08554545789957047...\n",
      "1/11100 Train Accuracy: 0.08540540188550949...\n",
      "1/11200 Train Accuracy: 0.08540178835391998...\n",
      "1/11300 Train Accuracy: 0.08522123843431473...\n",
      "1/11400 Train Accuracy: 0.08521930128335953...\n",
      "1/11500 Train Accuracy: 0.08500000089406967...\n",
      "1/11600 Train Accuracy: 0.08474137634038925...\n",
      "1/11700 Train Accuracy: 0.08431623876094818...\n",
      "1/11800 Train Accuracy: 0.08419491350650787...\n",
      "1/11900 Train Accuracy: 0.08411764353513718...\n",
      "1/12000 Train Accuracy: 0.08399999886751175...\n",
      "1/12100 Train Accuracy: 0.08384297788143158...\n",
      "1/12200 Train Accuracy: 0.08356557041406631...\n",
      "1/12300 Train Accuracy: 0.08321138471364975...\n",
      "1/12400 Train Accuracy: 0.08298387378454208...\n",
      "1/12500 Train Accuracy: 0.08284000307321548...\n",
      "1/12600 Train Accuracy: 0.08277777582406998...\n",
      "1/12700 Train Accuracy: 0.08248031139373779...\n",
      "1/12800 Train Accuracy: 0.08234374970197678...\n",
      "1/12900 Train Accuracy: 0.0820930227637291...\n",
      "1/13000 Train Accuracy: 0.08230768889188766...\n",
      "1/13100 Train Accuracy: 0.08232824504375458...\n",
      "1/13200 Train Accuracy: 0.08231060951948166...\n",
      "1/13300 Train Accuracy: 0.0825187936425209...\n",
      "1/13400 Train Accuracy: 0.0822761207818985...\n",
      "1/13500 Train Accuracy: 0.08222222328186035...\n",
      "1/13600 Train Accuracy: 0.08213235437870026...\n",
      "1/13700 Train Accuracy: 0.08204379677772522...\n",
      "1/13800 Train Accuracy: 0.08184782415628433...\n",
      "1/13900 Train Accuracy: 0.08187050372362137...\n",
      "1/14000 Train Accuracy: 0.08196428418159485...\n",
      "1/14100 Train Accuracy: 0.08187942951917648...\n",
      "1/14200 Train Accuracy: 0.08204225450754166...\n",
      "1/14300 Train Accuracy: 0.08202797174453735...\n",
      "1/14400 Train Accuracy: 0.08222222328186035...\n",
      "1/14500 Train Accuracy: 0.08213792741298676...\n",
      "1/14600 Train Accuracy: 0.08205479383468628...\n",
      "1/14700 Train Accuracy: 0.08180271834135056...\n",
      "1/14800 Train Accuracy: 0.08192567527294159...\n",
      "1/14900 Train Accuracy: 0.08218120783567429...\n",
      "1/15000 Train Accuracy: 0.08206667006015778...\n",
      "1/15100 Train Accuracy: 0.08192052692174911...\n",
      "1/15200 Train Accuracy: 0.08200658112764359...\n",
      "1/15300 Train Accuracy: 0.08179738372564316...\n",
      "1/15400 Train Accuracy: 0.08159090578556061...\n",
      "1/15500 Train Accuracy: 0.08145160973072052...\n",
      "1/15600 Train Accuracy: 0.08128204941749573...\n",
      "1/15700 Train Accuracy: 0.08098725974559784...\n",
      "1/15800 Train Accuracy: 0.08072784543037415...\n",
      "1/15900 Train Accuracy: 0.08062893152236938...\n",
      "1/16000 Train Accuracy: 0.08034375309944153...\n",
      "1/16100 Train Accuracy: 0.07999999821186066...\n"
     ]
    }
   ],
   "source": [
    "# Define Inputs and Hyperparameters\n",
    "lstm_sizes = [128,64]\n",
    "vocab_size = len(word2idx)  #add one for padding\n",
    "embed_size = config[\"emb_dim\"]\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    build_and_train_network(lstm_sizes, vocab_size, embed_size, epochs, batch_size,\n",
    "                            learning_rate, keep_prob, train,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.concatenate(train.loc[5:10][\"WordIDList\"].values,axis = 0).shape\n",
    "#np.array([[1,0] if item[0] == 0 else [0,1] for item in train[[\"target\"]].head(12).values]).shape\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    test_network('checkpoints', batch_size, test_x, test_y)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
